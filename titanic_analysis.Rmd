---
title: "Titanic Dataset"
output: html_notebook
---


```{r setup, include=FALSE}
packages <- c("tidyverse", "GGally", "broom", "modelr")
new_packages = newPackages <- packages[!(packages %in% as.character(installed.packages()[,"Package"]))]
if(length(newPackages)) install.packages(newPackages)
lapply(packages,require,character.only=T)

theme_set(theme_minimal())
```

# Titanic Competition: Kaggle

The objective is to use supervised classifier algorithms to determine a binary outcome. The data is the titanic dataset, information on passenges who embarked on the famous journey from Southampton (UK) to Cherbourg (France) to Queenstown (Ireland) with the final destination New York. 1502 out of 2224 passengers died. We aim to predict which passengers are most likely to die, given the information we have in the dataset.

## Exploratory Analysis

First produce scatter plots to see how each independent variable is related to the dependent variable.

```{r, echo=FALSE}
train = read_csv("~/Downloads/train.csv")

train$Sex = as.factor(train$Sex)
train$Survived = as.factor(train$Survived)

train %>% 
  select(-Name, -PassengerId) %>%
  gather(key, value, -Survived) %>%
  ggplot(aes(x = value, y = Survived)) + 
  geom_jitter() +
  facet_wrap(~key, scales = "free")
```

## Logistic Regression

We should first split the training set into 80% training and 20% test:

```{r}
## Selects a random 80% sample of the training set
indices = sample(nrow(train), replace = FALSE, size = floor(nrow(train) * 0.8))
training_split = train[indices,]
test_split = train[-indices,]
```


The simplest model is a logistic regression model, let's make a model with just Sex.

```{r}
logistic_fit = glm(Survived ~ Sex, data = training_split, family = "binomial")
tidy(logistic_fit)
```
Now we have an equation for the simple logistic regression model

```{r}
## attempt at using modelr, need to transform using the linking function "logit" to transform the log-odds to the probability of survival
survived = test_split %>%
  select(Sex, Survived, PassengerId) %>%
  add_predictions(logistic_fit) %>%
  mutate(pred_survived = make.link("logit")$linkinv(pred))
```
Now we can calculate an accuracy on the test set using the simple logistic regression model:

```{r}
survived %>% 
  mutate(pred_survived = ifelse(pred_survived > 0.5, 1, 0)) %>%
  summarise(accuracy = 1 - mean(Survived != pred_survived))
```

79.8% accuracy with only one predictor. 

Abstract this into a function:

```{r}
accuracy_logistic = function(test, model) {
  test %>%
    na.omit() %>%
    add_predictions(model) %>%
    mutate(pred_survived = ifelse(make.link("logit")$linkinv(pred) > 0.5, 1, 0)) %>%
    summarise(accuracy = 1 - mean(Survived != pred_survived))
}
```


Let's add some more variables to the model:

```{r}
logistic_fit_2 = glm(Survived ~ Sex + Age + Pclass, data = training_split %>% na.omit(), family = binomial(link='logit'))
accuracy_logistic(test_split, logistic_fit_2)
```
83%...

## Random Forest


